# -*- coding: utf-8 -*-
"""Uber Price Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hydbGPZi9WBCMKKYS0ggn3QDBG7gZP56
"""

''' In this project, I will be using machine learning to create a regression model that will the price of a uber 
    ride based on parameters such as pick up time, the pick up and drop off location, datetime, and passenger count.
    I will be using the XGBoost to build the regression model.
    I obtained this dataset on Kaggle (https://www.kaggle.com/datasets/yasserh/uber-fares-dataset)
'''

# We first import the required libraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import xgboost

# Now, we import our dataset into a Pandas dataframe

df = pd.read_csv('/content/uber.csv')

# We look for general information about our data. We see that we have some object data types which we will deal with. 
df.info()

# First, we convert the pickup_datetime to the datetime datatype

df.pickup_datetime = pd.to_datetime(df.pickup_datetime)

# Now, we drop duplicates rows 

df.drop_duplicates()

# We extract the hour, day and month from the datetime column

df['hour'] = df.pickup_datetime.dt.hour
df['day'] = df.pickup_datetime.dt.day
df['month'] = df.pickup_datetime.dt.month

# Now that we have the hour, we encode this column with the cos and sin function to preserve the cyclical nature of hour.

df['sin_hour'] = np.sin(2*np.pi*df.hour/24)
df['cos_hour'] = np.cos(2*np.pi*df.hour/24)

# Similarly, we perform cos and sin encoding for our day and month

df['sin_day'] = np.sin(2*np.pi*df.hour/30)
df['cos_day'] = np.cos(2*np.pi*df.hour/30)

df['sin_month'] = np.sin(2*np.pi*df.hour/12)
df['cos_month'] = np.cos(2*np.pi*df.hour/12)

# Now that we have the cos and sin encodings we can drop the following columns: 
# Unnamed 0, key, pickup_datetime, hour, day, and month

df = df.drop(['Unnamed: 0', 'key', 'pickup_datetime', 'hour', 'day', 'month'], axis=1)

# We also drop the one row that has an empty entry

df = df.dropna()
df.reset_index(drop=True)

# Now that we have dealt with our data, we can start doing some further exploration.
# We will create a heatmap to see the correlation between the features.

plt.figure(figsize=(15,8))
sns.heatmap(df.corr(), annot=True, cmap="YlGnBu")

# We see that while there are strong correlations between the pickup and dropoff features, we cannot drop
# these columns since the distance travelled in a ride should strongly determine the fare amount.

# To further visualize the relationship between pickup_longitude and dropoff_longitude, let's create a scatterplot 
plt.figure(figsize=(20,10))
sns.relplot(x='dropoff_longitude', y='pickup_longitude', hue='fare_amount', data=df, palette='viridis')
plt.xlim(-75,-73)
plt.ylim(-75,-73)

# Now that we have explored, cleaned and visualized our data, we are ready to build our model
# But first, we need to create our training and testing sets

X = df.drop(['fare_amount'], axis=1)
y = df['fare_amount']

# Since we have around 200,000 samples, a cross-validation set with 4000 samples is sufficient

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.02, random_state=42)

# Now we scale our data so that our model can train more efficiently during gradient descent.

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Now we are ready to build our model. We will be using XGBoost regression.

import xgboost

regressor = xgboost.XGBRegressor()

regressor.fit(X_train, y_train)

# We will compute the accuracy of our model

y_pred = regressor.predict(X_val)
from sklearn.metrics import r2_score
score = r2_score(y_val, y_pred)
print("The accuracy of our model is {}%".format(round(score, 2) *100))

# Great! We have achieved an accuracy of 75% which is a solid and realistic percentage for a regression model

# Finally, we will take advantage of XGboost feature importance function to verify our claim that
# the pick up and dropoff features are the most significant features in the model
plt.figure(figsize=(20,8))
plt.bar(X.columns, regressor.feature_importances_)